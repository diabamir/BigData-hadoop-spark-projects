# Big Data Projects â€“ Haifa University

This repository includes two Big Data homework assignments from the Big Data course at Haifa University. These assignments focus on the practical implementation of distributed data processing using **Hadoop MapReduce** and **Apache Spark**.

---

## ğŸ“¦ Homework 1 â€“ Running MapReduce with Hadoop

### ğŸ§  Objectives
- Set up a local Hadoop cluster using Docker and Docker Compose.
- Understand the architecture of HDFS and MapReduce.
- Implement and run classic MapReduce jobs.

### ğŸš€ Tasks Implemented
1. **Distributed Grep**: Search for a string across a distributed dataset.
2. **Word Length Counter**: Calculate frequency distribution of word lengths.
3. (Other MapReduce tasks can be added if implemented.)

### ğŸ›  Technologies Used
- Docker & Docker Compose
- Apache Hadoop (HDFS, YARN)
- Java
- Maven

---

## ğŸ“¦ Homework 2 â€“ Analytics with Apache Spark

### ğŸ§  Objectives
- Analyze a real-world dataset using Apache Spark.
- Apply Spark transformations and actions for text analytics.
- Handle stop words and generate meaningful 2-grams.

### ğŸ“Š Tasks Implemented
1. Count of **alphabetic characters** (case-insensitive).
2. Count of **words** (punctuation removed).
3. Generation and counting of **2-grams**.
4. **Top 20 most frequent words** excluding stop words.
5. **Stylistic summary** of Trumpâ€™s tweets.

### ğŸ›  Technologies Used
- Apache Spark
- Python (PySpark)
- Stop-words NLP library
- Jupyter Notebook or command-line Spark scripts

---

## ğŸ“ Project Structure

```bash
.
â”œâ”€â”€ homework1/                 # Hadoop + MapReduce (Java)
â”‚   â”œâ”€â”€ docker-hadoop/        # Docker Compose setup
â”‚   â”œâ”€â”€ src/                  # Java code (MapReduce implementations)
â”‚   â””â”€â”€ README.md             # Instructions
â”‚
â”œâ”€â”€ homework2/                # Spark analytics (Python / PySpark)
â”‚   â”œâ”€â”€ src/                  # Spark scripts or notebooks
â”‚   â””â”€â”€ trump_tweets.csv      # Dataset
â”‚
â””â”€â”€ README.md                 # This file
